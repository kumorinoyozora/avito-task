{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb123f9",
   "metadata": {},
   "source": [
    "Изобретать велосипед стоит только в том случае, когда хочешь разобраться, как он работает.\n",
    "\n",
    "Если же поставлена задача для решения, первое действие - поиск готовых вариантов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6198854",
   "metadata": {},
   "source": [
    "---\n",
    "### Загрузка данных для предикта и функции общего назначения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df5e7c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_no_spaces</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>куплюайфон14про</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ищудомвПодмосковье</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>сдаюквартирусмебельюитехникой</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>новыйдивандоставканедорого</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>отдамдаромкошку</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1000</td>\n",
       "      <td>Янеусну.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1001</td>\n",
       "      <td>Весна-яуженегреюпио.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>1002</td>\n",
       "      <td>Весна-скоровырастеттрава.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>1003</td>\n",
       "      <td>Весна-выпосмотрите,каккрасиво.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>1004</td>\n",
       "      <td>Весна-гдемояголова?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1005 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                  text_no_spaces\n",
       "0        0                 куплюайфон14про\n",
       "1        1              ищудомвПодмосковье\n",
       "2        2   сдаюквартирусмебельюитехникой\n",
       "3        3      новыйдивандоставканедорого\n",
       "4        4                 отдамдаромкошку\n",
       "...    ...                             ...\n",
       "1000  1000                        Янеусну.\n",
       "1001  1001            Весна-яуженегреюпио.\n",
       "1002  1002       Весна-скоровырастеттрава.\n",
       "1003  1003  Весна-выпосмотрите,каккрасиво.\n",
       "1004  1004             Весна-гдемояголова?\n",
       "\n",
       "[1005 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# read_csv ломается, так как в строках переменное число запятых\n",
    "rows = list()\n",
    "with open(\"dataset_1937770_3.txt\", encoding=\"utf-8\", mode='r') as file:\n",
    "    header = next(file).rstrip('\\n').split(',') \n",
    "    for line in file:\n",
    "        line = line.rstrip('\\n')\n",
    "        if not line:\n",
    "            continue\n",
    "        id_str, text_no_space = line.split(',', maxsplit=1) # только первая запятая\n",
    "        rows.append((int(id_str), text_no_space))\n",
    "\n",
    "df = pd.DataFrame(data=rows, columns=header)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37f8bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_spaces(text: str) -> list[int]:\n",
    "    '''\n",
    "    Поиск индексов пробелов\n",
    "\n",
    "    На вход подаётся строка с восстановленными пробелами\n",
    "    '''\n",
    "    spaces = list()\n",
    "    accumed_text = 0 # накопившийся обрезанный текст для правильной индексации\n",
    "    accumed_spaces = 0 # накопившиеся пробелы для приведения к слитной строке\n",
    "    while True:\n",
    "        space_idx = text.find(\" \")\n",
    "        if space_idx == -1: # условие выхода - ненаход\n",
    "            break\n",
    "        # логика подсказана здравым смыслом и опытом, тесты проходит\n",
    "        spaces.append(accumed_text + space_idx - accumed_spaces)\n",
    "\n",
    "        text = text[space_idx+1:] # +1 для пропуска найденного пробела\n",
    "\n",
    "        accumed_text += space_idx + 1 # см. коммент выше\n",
    "        accumed_spaces += 1 # очев\n",
    "    return spaces\n",
    "\n",
    "\n",
    "def make_submission(df, restored: pd.Series):\n",
    "    '''\n",
    "    restored - pd.Series из строк с восстановленными пробелами\n",
    "    '''\n",
    "\n",
    "    df[\"predicted_positions\"] = restored.apply(find_spaces).apply(str)\n",
    "    res = df.drop(columns=(\"text_no_spaces\"))\n",
    "    res.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6d992d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 10, 12, 16]\n"
     ]
    }
   ],
   "source": [
    "# тесты find_spaces\n",
    "\n",
    "text = \"ищу дом в Подмосковье\" # ищудомвПодмосковье [3, 6, 7]\n",
    "text = \"Весна - я уже не грею пио.\" # Весна-яуженегреюпио. [5, 6, 7, 10, 12, 16]\n",
    "print(find_spaces(text)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8409092a",
   "metadata": {},
   "source": [
    "---\n",
    "### Экспериментальная секция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d98cbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")\n",
    "\n",
    "# text = \"ищудомвПодмосковье\"\n",
    "\n",
    "# tokens = tokenizer.tokenize(text)\n",
    "# restored = str.join(\" \", tokens)\n",
    "\n",
    "# print(tokens)\n",
    "# print(restored)\n",
    "\n",
    "# for text in df.text_no_spaces:\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     tokens = [t.strip(\"#\") for t in tokens]\n",
    "#     restored = str.join(\" \", tokens)\n",
    "#     print(restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2a346d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# первый собранный рабочий вариант с Mean F1 = 49.502%\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\") \n",
    "\n",
    "tokenized = df['text_no_spaces'].apply(tokenizer.tokenize) # токенизация \n",
    "tokenized = tokenized.apply(lambda l: [s.strip(\"#\") for s in l]) # удаление \"##\" перед токенами\n",
    "restored = tokenized.apply(lambda l: str.join(' ', l)) # восстановление строк\n",
    "\n",
    "make_submission(df, restored)\n",
    "\n",
    "# проблема: слишком агрессивное разделение на токены\n",
    "# можно попробовать другие варианты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "986e3b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize\n",
    "\n",
    "\n",
    "def restore_with_razdel(text: str):\n",
    "    tokens = [t.text for t in tokenize(text)]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# for text in df.text_no_spaces:\n",
    "#     restored = restore_with_razdel(text)\n",
    "#     print(restored)\n",
    "\n",
    "# этот напротив не разделяет практически вообще русские слова\n",
    "# хорошо отделяет числа и английские слова, можно использовать как препроцессор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd916321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Koziev/rutokenizer\n",
    "\n",
    "import rutokenizer\n",
    "\n",
    "\n",
    "def restore_with_rutokenizer(text: str, tokenizer: rutokenizer.Tokenizer):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "tokenizer = rutokenizer.Tokenizer()\n",
    "tokenizer.load()\n",
    "\n",
    "# for text in df.text_no_spaces:\n",
    "#     restored = restore_with_rutokenizer(text, tokenizer)\n",
    "#     print(restored)\n",
    "\n",
    "# бесполезен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7f0a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовка данных для обучения sentencepiece\n",
    "\n",
    "with open(\"train.txt\", \"w\", encoding='utf-8') as train:\n",
    "    for line in df[\"text_no_spaces\"]:\n",
    "        # line = restore_with_razdel(line) # можно подключить razdel как препроцессор\n",
    "        train.write(line + \"\\n\")\n",
    "\n",
    "# с razdel'ом на глаз результат хуже, после препроцессинга spm начинает дробить английские слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f844df5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# обучение модели\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='train.txt', # файл, где каждая строка = один пример текста\n",
    "    model_prefix='mymodel',\n",
    "    vocab_size=8000,\n",
    "    character_coverage=1.0, # важно для кириллицы, чтобы не потерять буквы\n",
    "    model_type='bpe' # можно unigram попробовать как вариант\n",
    ")\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"mymodel.model\")\n",
    "\n",
    "# for text in df[\"text_no_spaces\"]:\n",
    "#     tokens = sp.encode(text, out_type=str)\n",
    "#     print(tokens)\n",
    "\n",
    "# показывает сбалансированное разделение даже на сырых тренировочных данных\n",
    "# не такой агрессивный, как Bert'ы, отделяет иногда неплохо\n",
    "# Mean F1 = 36.254%, всё ещё хуже самого первого решения\n",
    "# по-хорошему, нужно ещё что-то вроде пост-обработки\n",
    "\n",
    "\n",
    "# def restore_with_spm(text: str):\n",
    "#     tokens = sp.encode(text, out_type=str)\n",
    "#     tokens = [t.strip(\"▁\") for t in tokens]\n",
    "#     return str.join(\" \", tokens)\n",
    "\n",
    "# restored = df[\"text_no_spaces\"].apply(restore_with_spm)\n",
    "# make_submission(df, restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d8e19",
   "metadata": {},
   "source": [
    "---\n",
    "### Лучшее решение + препроцессинг + пост-обработка\n",
    "\n",
    "Идея: взять как препроцессор `razdel`, далее сегментацию производить с помощью `AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\")` по каждому токену, если он не является английским словом или числом, затем с тем же условием решать, склеивать токены обратно или нет, ориентируясь на внешний словарь."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678da88",
   "metadata": {},
   "source": [
    "Словарь взят отсюда: https://github.com/caffidev/russianwords/blob/main/utf-8/words_cases.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d38f5bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "\n",
    "\n",
    "class Segmenter:\n",
    "    def __init__(self, path):\n",
    "        with open(path, 'r', encoding=\"utf-8\") as file:\n",
    "            self.vocab = set(word.strip().lower() for word in file if word.strip())\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/sbert_large_nlu_ru\") \n",
    "\n",
    "        self._ascii_re = re.compile(r'^[\\x00-\\x7f]+$')\n",
    "        self._punct_before_re = re.compile(r'\\s+([.,!?;:])')\n",
    "        self._open_after_re = re.compile(r'([“\"\\'«(\\[{])\\s+')\n",
    "        self._close_before_re = re.compile(r'\\s+([”\"\\'»)\\]}])')\n",
    "    \n",
    "    def _is_ascii(self, token: str) -> bool:\n",
    "        return bool(self._ascii_re.match(token))\n",
    "        \n",
    "    def _build_from_subtokens(self, token: str, subtokens: list[str]) -> list[str]:\n",
    "        # склейка сабтокенов по словарю\n",
    "        buffer = \"\"\n",
    "        parts = list()\n",
    "        for st in subtokens:\n",
    "            piece = st.lstrip(\"#\")\n",
    "            if not piece: \n",
    "                continue\n",
    "            buffer += piece\n",
    "            if buffer.lower() in self.vocab:\n",
    "                parts.append(buffer)\n",
    "                buffer = \"\"\n",
    "        if buffer == \"\":\n",
    "            return parts # успешно разобран\n",
    "        \n",
    "        remained = buffer\n",
    "        tmp = list()\n",
    "        while remained:\n",
    "            matched = False\n",
    "            for l in range(len(remained), 0, -1):\n",
    "                pref = remained[:l]\n",
    "                if pref.lower() in self.vocab:\n",
    "                    tmp.append(pref)\n",
    "                    remained = remained[l:]\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                return None\n",
    "        parts.extend(tmp)\n",
    "        return parts\n",
    "    \n",
    "    def preprocessor(self, text: str) -> str:\n",
    "        # базовая сегментация razdel\n",
    "        tokens = [t.text for t in razdel_tokenize(text)]\n",
    "        text = \" \".join(tokens)\n",
    "\n",
    "        # удаление лишних пробелов перед знаками препинания и вокруг кавычек\n",
    "        text = self._punct_before_re.sub(r\"\\1\", text)\n",
    "        text = self._open_after_re.sub(r\"\\1\", text)\n",
    "        text = self._close_before_re.sub(r\"\\1\", text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def segment(self, text: str) -> str:\n",
    "        tokens = text.split()\n",
    "        out_tokens = list()\n",
    "\n",
    "        for token in tokens:\n",
    "            # англ / числа / пунктуация\n",
    "            if self._is_ascii(token):\n",
    "                out_tokens.append(token)\n",
    "                continue\n",
    "            \n",
    "            # токенизация \n",
    "            subtokens = self.tokenizer.tokenize(token)\n",
    "            parts = self._build_from_subtokens(token, subtokens) # попытка склейки слов по словарю\n",
    "            if parts is not None:\n",
    "                out_tokens.extend(parts)\n",
    "                continue\n",
    "            \n",
    "\n",
    "            # восстановление строки при частично удачной или неудачной попытке склейки\n",
    "            remained = token\n",
    "            tmp = list()\n",
    "            while remained:\n",
    "                matched = False\n",
    "                for l in range(len(remained), 0, -1):\n",
    "                    pref = remained[:l]\n",
    "                    if pref.lower() in self.vocab:\n",
    "                        tmp.append(pref)\n",
    "                        remained = remained[l:]\n",
    "                        matched = True\n",
    "                        break\n",
    "                if not matched:\n",
    "                    # сохраняем только остаток, а не весь токен\n",
    "                    tmp.append(remained)\n",
    "                    remained = \"\"\n",
    "            out_tokens.extend(tmp)\n",
    "\n",
    "        # финальная чистка\n",
    "        s = \" \".join(out_tokens)\n",
    "        s = self._punct_before_re.sub(r\"\\1\", s)\n",
    "        s = self._open_after_re.sub(r\"\\1\", s)\n",
    "        s = self._close_before_re.sub(r\"\\1\", s)\n",
    "\n",
    "        return s\n",
    "    \n",
    "    def process(self, text: str) -> str:\n",
    "        return self.segment(self.preprocessor(text))\n",
    "\n",
    "model = Segmenter(path=\"words_cases.txt\")\n",
    "\n",
    "\n",
    "# for text in df.text_no_spaces:\n",
    "#     restored = model.process(text)\n",
    "#     print(restored)\n",
    "\n",
    "\n",
    "restored = df['text_no_spaces'].apply(model.process)\n",
    "make_submission(df, restored)\n",
    "\n",
    "# Mean F1 = 59.659%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
